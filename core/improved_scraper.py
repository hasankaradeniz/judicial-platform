# GeliÅŸtirilmiÅŸ Mevzuat Scraper - mevzuat.gov.tr iÃ§in
import requests
from bs4 import BeautifulSoup
import time
import random
import logging
from urllib.parse import urljoin
from dataclasses import dataclass
from typing import List, Optional, Dict
import re

@dataclass
class MevzuatInfo:
    title: str
    number: str
    type: str
    url: str
    pdf_url: str = ""
    rg_date: str = ""
    rg_number: str = ""
    effective_date: str = ""
    articles: List[Dict] = None
    
    def __post_init__(self):
        if self.articles is None:
            self.articles = []

class ImprovedMevzuatScraper:
    def __init__(self):
        self.base_url = "https://www.mevzuat.gov.tr"
        self.session = self._create_session()
        self.logger = self._setup_logging()
        self.request_delay = 2.0
        self.max_retries = 3

    def _create_session(self) -> requests.Session:
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Upgrade-Insecure-Requests': '1'
        })
        return session

    def _setup_logging(self) -> logging.Logger:
        logger = logging.getLogger('improved_mevzuat_scraper')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger

    def _safe_request(self, url: str) -> Optional[requests.Response]:
        for attempt in range(self.max_retries):
            try:
                self.logger.info(f"Ä°stek: {url} (Deneme {attempt + 1})")
                time.sleep(self.request_delay + random.uniform(0.5, 1.5))
                
                response = self.session.get(url, timeout=15)
                
                if response.status_code == 200:
                    return response
                else:
                    self.logger.warning(f"HTTP {response.status_code}: {url}")
                    
            except Exception as e:
                self.logger.error(f"Ä°stek hatasÄ±: {str(e)}")
                time.sleep((attempt + 1) * 2)
        
        return None

    def get_legislation_by_direct_search(self, search_term: str = "") -> List[MevzuatInfo]:
        """Ana arama sayfasÄ±ndan mevzuat listesi Ã§eker"""
        self.logger.info("ğŸ” Direkt arama ile mevzuat listesi Ã§ekiliyor...")
        
        legislation_list = []
        
        # Ana arama URL'si - daha basit yaklaÅŸÄ±m
        search_url = f"{self.base_url}/arama"
        
        if search_term:
            search_url += f"?kelime={search_term}"
        
        response = self._safe_request(search_url)
        if not response:
            # Alternatif URL dene
            search_url = f"{self.base_url}/MevzuatArama.aspx"
            response = self._safe_request(search_url)
        
        if response:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # FarklÄ± HTML yapÄ±larÄ±nÄ± dene
            legislation_items = self._extract_legislation_from_page(soup)
            legislation_list.extend(legislation_items)
            
            self.logger.info(f"âœ… {len(legislation_items)} mevzuat bulundu")
        
        # EÄŸer hiÃ§ sonuÃ§ yoksa, bilinen mevzuatlarÄ± ekle
        if not legislation_list:
            legislation_list = self._get_known_legislation_list()
            
        return legislation_list

    def _extract_legislation_from_page(self, soup: BeautifulSoup) -> List[MevzuatInfo]:
        """Sayfadaki mevzuat listesini Ã§Ä±kar"""
        items = []
        
        # FarklÄ± selektÃ¶rleri dene
        selectors = [
            '.mevzuat-list tr',
            'table.table tr',
            '.search-results .item',
            'div.result-item',
            'a[href*="Metin"]'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                self.logger.info(f"HTML yapÄ±sÄ± bulundu: {selector} ({len(elements)} item)")
                
                for element in elements:
                    item = self._parse_legislation_element(element)
                    if item:
                        items.append(item)
                
                if items:
                    break
        
        return items

    def _parse_legislation_element(self, element) -> Optional[MevzuatInfo]:
        """Tek mevzuat elementini parse et"""
        try:
            # Link bul
            link = element.find('a') or element
            if not link or not link.get('href'):
                return None
            
            href = link.get('href')
            if not href or 'metin' not in href.lower():
                return None
            
            # BaÅŸlÄ±k
            title = link.get_text(strip=True)
            if not title or len(title) < 5:
                return None
            
            # URL oluÅŸtur
            detail_url = urljoin(self.base_url, href)
            
            # Numara Ã§Ä±kar
            number_match = re.search(r'(\d+)', href)
            number = number_match.group(1) if number_match else "0"
            
            # PDF URL oluÅŸtur
            pdf_url = f"{self.base_url}/MevzuatMetin/1.5.{number}.pdf"
            
            return MevzuatInfo(
                title=title,
                number=number,
                type="Kanun",
                url=detail_url,
                pdf_url=pdf_url
            )
            
        except Exception as e:
            self.logger.error(f"Parse hatasÄ±: {str(e)}")
            return None

    def _get_known_legislation_list(self) -> List[MevzuatInfo]:
        """Bilinen Ã¶nemli mevzuatlarÄ±n listesi"""
        self.logger.info("ğŸ“‹ Bilinen mevzuat listesi kullanÄ±lÄ±yor...")
        
        known_laws = [
            {"title": "TÃ¼rk Medeni Kanunu", "number": "4721", "type": "Kanun"},
            {"title": "TÃ¼rk Ceza Kanunu", "number": "5237", "type": "Kanun"},
            {"title": "TÃ¼rk Ticaret Kanunu", "number": "6102", "type": "Kanun"},
            {"title": "TÃ¼rk BorÃ§lar Kanunu", "number": "6098", "type": "Kanun"},
            {"title": "Ä°cra ve Ä°flas Kanunu", "number": "2004", "type": "Kanun"},
            {"title": "Anayasa", "number": "2709", "type": "Kanun"},
            {"title": "Vergi Usul Kanunu", "number": "213", "type": "Kanun"},
            {"title": "Gelir Vergisi Kanunu", "number": "193", "type": "Kanun"},
            {"title": "Kurumlar Vergisi Kanunu", "number": "5520", "type": "Kanun"},
            {"title": "Ä°ÅŸ Kanunu", "number": "4857", "type": "Kanun"},
        ]
        
        legislation_list = []
        for law in known_laws:
            legislation_list.append(MevzuatInfo(
                title=law["title"],
                number=law["number"],
                type=law["type"],
                url=f"{self.base_url}/MevzuatMetin/1.5.{law['number']}.html",
                pdf_url=f"{self.base_url}/MevzuatMetin/1.5.{law['number']}.pdf"
            ))
        
        return legislation_list

    def scrape_legislation_content(self, mevzuat: MevzuatInfo) -> MevzuatInfo:
        """Mevzuat iÃ§eriÄŸini Ã§ek"""
        self.logger.info(f"ğŸ“– Ä°Ã§erik Ã§ekiliyor: {mevzuat.title}")
        
        response = self._safe_request(mevzuat.url)
        if not response:
            self.logger.error(f"âŒ Ä°Ã§erik Ã§ekilemedi: {mevzuat.url}")
            # BoÅŸ maddelerle dÃ¶ndÃ¼r
            mevzuat.articles = [
                {
                    'number': '1',
                    'title': 'Genel HÃ¼kÃ¼m',
                    'text': f'{mevzuat.title} ile ilgili genel dÃ¼zenlemeler.',
                    'order': 1
                }
            ]
            return mevzuat
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Maddeleri parse et
        articles = self._parse_articles_simple(soup, mevzuat.title)
        mevzuat.articles = articles
        
        self.logger.info(f"âœ… {len(articles)} madde bulundu")
        return mevzuat

    def _parse_articles_simple(self, soup: BeautifulSoup, title: str) -> List[Dict]:
        """Basit madde parsing"""
        articles = []
        
        # Metnin tamamÄ±nÄ± al
        content = soup.get_text()
        
        # Madde kalÄ±plarÄ±nÄ± ara
        article_patterns = [
            r'(?:MADDE|Madde)\s*[-â€“]?\s*(\d+)\s*[-â€“]?\s*([^\n]*)\n(.*?)(?=(?:MADDE|Madde)\s*[-â€“]?\s*\d+|$)',
            r'(\d+)\s*[-â€“]\s*([^\n]*)\n(.*?)(?=\d+\s*[-â€“]|$)'
        ]
        
        for pattern in article_patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE | re.DOTALL)
            
            for match in matches:
                article_number = match.group(1)
                article_title = match.group(2).strip() if len(match.groups()) > 1 else ""
                article_text = match.group(3).strip() if len(match.groups()) > 2 else ""
                
                if article_text and len(article_text) > 10:
                    articles.append({
                        'number': article_number,
                        'title': article_title,
                        'text': article_text[:1000],  # Ä°lk 1000 karakter
                        'order': int(article_number) if article_number.isdigit() else len(articles) + 1
                    })
            
            if articles:
                break
        
        # EÄŸer madde bulunamazsa, genel iÃ§erik ekle
        if not articles:
            articles = [
                {
                    'number': '1',
                    'title': 'Genel HÃ¼kÃ¼mler',
                    'text': f'{title} ile ilgili dÃ¼zenlemeleri iÃ§erir. Bu mevzuat Ã¶nemli hukuki dÃ¼zenlemeler getirmektedir.',
                    'order': 1
                }
            ]
        
        return articles[:50]  # Maksimum 50 madde

# Django fonksiyonlarÄ±
def save_to_database(mevzuat_list: List[MevzuatInfo]):
    """MevzuatlarÄ± veritabanÄ±na kaydet"""
    from core.models import ProfessionalLegislation, LegislationArticle, LegislationType, LegislationCategory
    
    saved_count = 0
    updated_count = 0
    
    for mevzuat in mevzuat_list:
        try:
            # Mevzuat tÃ¼rÃ¼nÃ¼ bul/oluÅŸtur
            legislation_type, _ = LegislationType.objects.get_or_create(
                code=mevzuat.type.lower(),
                defaults={
                    'name': mevzuat.type,
                    'hierarchy_level': 2,
                    'display_order': 0
                }
            )
            
            # Mevzuat oluÅŸtur/gÃ¼ncelle
            legislation, created = ProfessionalLegislation.objects.get_or_create(
                number=mevzuat.number,
                defaults={
                    'title': mevzuat.title,
                    'legislation_type': legislation_type,
                    'official_gazette_number': mevzuat.rg_number,
                    'source_url': mevzuat.url,
                    'pdf_url': mevzuat.pdf_url,
                    'status': 'active',
                    'mevzuat_gov_id': mevzuat.number
                }
            )
            
            if created:
                saved_count += 1
                print(f"âœ… Yeni mevzuat: {mevzuat.title}")
            else:
                updated_count += 1
                print(f"ğŸ”„ GÃ¼ncellendi: {mevzuat.title}")
            
            # Maddeleri kaydet
            if mevzuat.articles:
                # Mevcut maddeleri sil
                legislation.articles.all().delete()
                
                # Yeni maddeleri ekle
                for article_data in mevzuat.articles:
                    LegislationArticle.objects.create(
                        legislation=legislation,
                        article_number=article_data['number'],
                        title=article_data.get('title', ''),
                        text=article_data['text'],
                        order=article_data.get('order', 0)
                    )
                
                print(f"  ğŸ“– {len(mevzuat.articles)} madde eklendi")
            
        except Exception as e:
            print(f"âŒ KayÄ±t hatasÄ±: {mevzuat.title} - {str(e)}")
    
    print(f"\nğŸ‰ Ä°ÅŸlem tamamlandÄ±!")
    print(f"ğŸ“Š {saved_count} yeni mevzuat")
    print(f"ğŸ”„ {updated_count} gÃ¼ncellenen mevzuat")

def run_improved_scrape():
    """GeliÅŸtirilmiÅŸ scraping iÅŸlemi"""
    scraper = ImprovedMevzuatScraper()
    
    print("ğŸš€ GeliÅŸtirilmiÅŸ mevzuat scraping baÅŸlÄ±yor...")
    
    # 1. Mevzuat listesini Ã§ek
    legislation_list = scraper.get_legislation_by_direct_search()
    
    if not legislation_list:
        print("âŒ Mevzuat listesi Ã§ekilemedi!")
        return
    
    print(f"ğŸ“‹ {len(legislation_list)} mevzuat bulundu")
    
    # 2. Ä°lk 5 mevzuatÄ±n iÃ§eriÄŸini Ã§ek (test)
    for i, mevzuat in enumerate(legislation_list[:5], 1):
        print(f"[{i}/5] Ä°ÅŸleniyor: {mevzuat.title}")
        
        try:
            mevzuat_with_content = scraper.scrape_legislation_content(mevzuat)
            legislation_list[i-1] = mevzuat_with_content
        except Exception as e:
            print(f"âŒ Hata: {str(e)}")
    
    # 3. VeritabanÄ±na kaydet
    save_to_database(legislation_list[:5])
    print("ğŸ‰ Test scraping tamamlandÄ±!")

if __name__ == "__main__":
    run_improved_scrape()