# core/simple_article_search.py

import requests
import time
import random
import hashlib
from django.core.cache import cache
from datetime import datetime


class SimpleArticleSearcher:
    """Basit ve gÃ¼venilir makale arama sistemi - Sadece API'ler"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Academic Research Tool (contact@research.com)',
            'Accept': 'application/json',
            'Content-Type': 'application/json'
        })
    
    def search_articles(self, query, limit=20, page=1):
        """Ana arama fonksiyonu - paginated"""
        offset = (page - 1) * 10  # 10'arlÄ± sayfalar
        
        # Cache kontrolÃ¼ (page'e gÃ¶re)
        cached_result = None
        try:
            cache_key = f'simple_articles_{hashlib.md5(query.encode()).hexdigest()}_page_{page}'
            cached_result = cache.get(cache_key)
            if cached_result:
                print(f"ğŸ“š Cache'den sayfa {page} iÃ§in {len(cached_result)} makale dÃ¶ndÃ¼rÃ¼ldÃ¼")
                return cached_result
        except Exception as e:
            print(f"Cache hatasÄ± (devam ediliyor): {e}")
            cached_result = None
        
        print(f"ğŸ” '{query}' sayfa {page} iÃ§in makale aranÄ±yor...")
        results = []
        
        # Her sayfa iÃ§in daha fazla sonuÃ§ Ã§ek
        crossref_rows = 50 + offset  # Daha fazla sonuÃ§ al
        doaj_rows = 30 + offset
        
        # 1. CrossRef API (En gÃ¼venilir)
        print(f"ğŸ” CrossRef API'sine baÄŸlanÄ±yor (sayfa {page})...")
        crossref_results = self._search_crossref(query, crossref_rows, offset)
        if crossref_results:
            results.extend(crossref_results)
            print(f"âœ… CrossRef: {len(crossref_results)} makale")
        else:
            print(f"âŒ CrossRef: 0 makale")
        
        # 2. DOAJ API (AÃ§Ä±k eriÅŸim)
        print(f"ğŸ” DOAJ API'sine baÄŸlanÄ±yor (sayfa {page})...")
        doaj_results = self._search_doaj(query, doaj_rows, offset)
        if doaj_results:
            results.extend(doaj_results)
            print(f"âœ… DOAJ: {len(doaj_results)} makale")
        else:
            print(f"âŒ DOAJ: 0 makale")
        
        # 3. SonuÃ§larÄ± temizle ve sÄ±rala
        unique_results = self._clean_results(results, query)
        
        # Pagination: Sadece 10 makale dÃ¶ndÃ¼r
        start_idx = offset % len(unique_results) if unique_results else 0
        end_idx = start_idx + 10
        final_results = unique_results[start_idx:end_idx] if unique_results else []
        
        # Cache'e kaydet (2 saat) - gÃ¼venli ÅŸekilde
        try:
            cache.set(cache_key, final_results, 7200)
        except Exception as e:
            print(f"Cache kaydetme hatasÄ± (devam ediliyor): {e}")
        
        print(f"ğŸ“– Toplam {len(final_results)} makale bulundu")
        return final_results
    
    def _search_crossref(self, query, rows, offset=0):
        """CrossRef API ile arama - paginated"""
        results = []
        try:
            url = "https://api.crossref.org/works"
            params = {
                'query': query,
                'rows': rows,
                'offset': offset,
                'sort': 'relevance',
                'filter': 'type:journal-article'
            }
            
            print(f"ğŸ“¡ CrossRef URL: {url}")
            print(f"ğŸ“¡ CrossRef params: {params}")
            
            response = self.session.get(url, params=params, timeout=15)
            print(f"ğŸ“¡ CrossRef response status: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                items = data.get('message', {}).get('items', [])
                print(f"ğŸ“¡ CrossRef raw items: {len(items)}")
                
                for i, item in enumerate(items):
                    article = self._parse_crossref_item(item)
                    if article:
                        results.append(article)
                        print(f"  âœ… Parse edildi: {article['title'][:50]}...")
                    else:
                        print(f"  âŒ Parse edilemedi: {i+1}")
            else:
                print(f"âŒ CrossRef HTTP error: {response.status_code}")
                        
            time.sleep(0.5)  # Rate limiting
            
        except Exception as e:
            print(f"âŒ CrossRef API hatasÄ±: {e}")
        
        print(f"ğŸ“Š CrossRef final results: {len(results)}")
        return results
    
    def _search_doaj(self, query, page_size, offset=0):
        """DOAJ API ile arama - paginated"""
        results = []
        try:
            url = "https://doaj.org/api/search/articles"
            page_num = (offset // 10) + 1  # DOAJ page numbering
            params = {
                'query': query,
                'pageSize': min(page_size, 50),  # DOAJ limit
                'page': page_num,
                'sort': 'score:desc'
            }
            
            response = self.session.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                items = data.get('results', [])
                
                for item in items:
                    article = self._parse_doaj_item(item)
                    if article:
                        results.append(article)
                        
            time.sleep(0.5)  # Rate limiting
            
        except Exception as e:
            print(f"DOAJ API hatasÄ±: {e}")
        
        return results
    
    def _parse_crossref_item(self, item):
        """CrossRef makalesini parse et"""
        try:
            title = item.get('title', [''])[0] if item.get('title') else ''
            if not title or len(title) < 10:
                return None
            
            # Yazarlar
            authors = []
            if item.get('author'):
                for author in item['author'][:3]:  # Ä°lk 3 yazar
                    given = author.get('given', '')
                    family = author.get('family', '')
                    if given and family:
                        authors.append(f"{given} {family}")
            authors_str = ', '.join(authors) if authors else 'Yazar bilgisi yok'
            
            # Dergi
            journal = ''
            if item.get('container-title'):
                journal = item['container-title'][0]
            
            # YÄ±l
            year = ''
            if item.get('published-print', {}).get('date-parts'):
                year = str(item['published-print']['date-parts'][0][0])
            elif item.get('published-online', {}).get('date-parts'):
                year = str(item['published-online']['date-parts'][0][0])
            
            # DOI ve linkler
            doi = item.get('DOI', '')
            detail_link = f"https://doi.org/{doi}" if doi else ''
            
            # Ã–zet (varsa) - CrossRef'den gelmeyebilir, sonradan Ã§ekilecek
            abstract = item.get('abstract', '')
            if abstract and len(abstract) > 300:
                abstract = abstract[:300] + "..."
            elif not abstract:
                # Ã–zet sonradan Ã§ekilecek
                abstract = 'Ã–zet yÃ¼kleniyor...'
            
            return {
                'id': f"crossref_{doi.replace('/', '_')}" if doi else f"crossref_{hash(title)%100000}",
                'title': title,
                'authors': authors_str,
                'journal': journal or 'Academic Journal',
                'year': year or '2024',
                'abstract': abstract,
                'detail_link': detail_link,
                'pdf_link': detail_link,
                'real_pdf': detail_link,
                'doi': doi,
                'source': 'CrossRef',
                'source_icon': 'ğŸ”¬'
            }
            
        except Exception:
            return None
    
    def _parse_doaj_item(self, item):
        """DOAJ makalesini parse et"""
        try:
            bibjson = item.get('bibjson', {})
            title = bibjson.get('title', '')
            
            if not title or len(title) < 10:
                return None
            
            # Yazarlar
            authors = []
            if bibjson.get('author'):
                for author in bibjson['author'][:3]:  # Ä°lk 3 yazar
                    name = author.get('name', '')
                    if name:
                        authors.append(name)
            authors_str = ', '.join(authors) if authors else 'Yazar bilgisi yok'
            
            # Dergi
            journal = bibjson.get('journal', {}).get('title', 'Open Access Journal')
            
            # YÄ±l
            year = str(bibjson.get('year', '2024'))
            
            # DOI ve linkler
            doi = ''
            detail_link = ''
            pdf_link = ''
            
            if bibjson.get('identifier'):
                for identifier in bibjson['identifier']:
                    if identifier.get('type') == 'doi':
                        doi = identifier.get('id', '')
                        detail_link = f"https://doi.org/{doi}"
                        break
            
            if bibjson.get('link'):
                for link in bibjson['link']:
                    url = link.get('url', '')
                    link_type = link.get('type', '').lower()
                    
                    if 'pdf' in link_type or 'fulltext' in link_type:
                        pdf_link = url
                        if not detail_link:
                            detail_link = url
                        break
            
            # Ã–zet
            abstract = bibjson.get('abstract', 'Bu aÃ§Ä±k eriÅŸim makalesi iÃ§in Ã¶zet mevcut deÄŸil')
            if len(abstract) > 300:
                abstract = abstract[:300] + "..."
            
            return {
                'id': f"doaj_{doi.replace('/', '_')}" if doi else f"doaj_{hash(title)%100000}",
                'title': title,
                'authors': authors_str,
                'journal': journal,
                'year': year,
                'abstract': abstract,
                'detail_link': detail_link or pdf_link,
                'pdf_link': pdf_link or detail_link,
                'real_pdf': pdf_link or detail_link,
                'doi': doi,
                'source': 'DOAJ',
                'source_icon': 'ğŸ“–'
            }
            
        except Exception:
            return None
    
    def _clean_results(self, results, query):
        """SonuÃ§larÄ± temizle ve sÄ±rala"""
        seen_titles = set()
        unique_results = []
        
        for article in results:
            title = article.get('title', '').lower().strip()
            
            # Ã‡ok kÄ±sa baÅŸlÄ±klarÄ± filtrele
            if len(title) < 10:
                continue
            
            # Tekrar edenleri filtrele
            if title in seen_titles:
                continue
            
            # Relevance kontrolÃ¼
            query_words = query.lower().split()
            title_words = title.split()
            
            # En az bir kelime eÅŸleÅŸmesi olmalÄ±
            has_match = any(
                any(q_word in t_word for t_word in title_words)
                for q_word in query_words
            )
            
            if has_match:
                seen_titles.add(title)
                unique_results.append(article)
        
        return unique_results
    
    def fetch_abstract_from_url(self, url):
        """URL'den Ã¶zet Ã§ekme fonksiyonu"""
        try:
            print(f"ğŸ“„ Ã–zet Ã§ekiliyor: {url}")
            
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Dergipark Ã¶zet Ã§ekme
                if 'dergipark.org.tr' in url:
                    return self._extract_dergipark_abstract(soup)
                
                # DOI/CrossRef Ã¶zet Ã§ekme
                elif 'doi.org' in url:
                    return self._extract_doi_abstract(soup)
                
                # Genel Ã¶zet Ã§ekme
                else:
                    return self._extract_general_abstract(soup)
                    
        except Exception as e:
            print(f"âŒ Ã–zet Ã§ekme hatasÄ±: {e}")
            return None
        
        return None
    
    def _extract_dergipark_abstract(self, soup):
        """Dergipark'tan Ã¶zet Ã§ek"""
        try:
            # Dergipark'ta "Ã–z" baÅŸlÄ±ÄŸÄ± altÄ±ndaki iÃ§erik
            abstract_selectors = [
                'div.article-abstract-tr',
                'div.article-abstract',
                'div[id*="abstract"]',
                'div[class*="abstract"]',
                '.article-summary',
                '.abstract-content',
                'div:contains("Ã–z") + div',
                'p:contains("Ã–z") + p'
            ]
            
            for selector in abstract_selectors:
                if ':contains(' in selector:
                    # BeautifulSoup doesn't support :contains, use find instead
                    if 'Ã–z' in selector:
                        oz_element = soup.find(text=lambda text: text and 'Ã–z' in text)
                        if oz_element:
                            parent = oz_element.parent
                            if parent:
                                next_element = parent.find_next_sibling()
                                if next_element:
                                    abstract_text = next_element.get_text(strip=True)
                                    if len(abstract_text) > 50:
                                        return abstract_text[:500] + "..." if len(abstract_text) > 500 else abstract_text
                else:
                    element = soup.select_one(selector)
                    if element:
                        abstract_text = element.get_text(strip=True)
                        if len(abstract_text) > 50:
                            return abstract_text[:500] + "..." if len(abstract_text) > 500 else abstract_text
            
            # Alternatif: TÃ¼m p tag'lerinde Ã¶zet ara
            for p in soup.find_all('p'):
                text = p.get_text(strip=True)
                if len(text) > 100 and any(keyword in text.lower() for keyword in ['amaÃ§', 'Ã§alÄ±ÅŸma', 'araÅŸtÄ±rma', 'sonuÃ§']):
                    return text[:500] + "..." if len(text) > 500 else text
                    
        except Exception as e:
            print(f"âŒ Dergipark Ã¶zet Ã§ekme hatasÄ±: {e}")
        
        return None
    
    def _extract_doi_abstract(self, soup):
        """DOI sayfasÄ±ndan Ã¶zet Ã§ek"""
        try:
            abstract_selectors = [
                'div.abstractSection',
                'div.abstract',
                'section.abstract',
                '.article-abstract',
                '.abstract-content'
            ]
            
            for selector in abstract_selectors:
                element = soup.select_one(selector)
                if element:
                    abstract_text = element.get_text(strip=True)
                    if len(abstract_text) > 50:
                        return abstract_text[:500] + "..." if len(abstract_text) > 500 else abstract_text
                        
        except Exception as e:
            print(f"âŒ DOI Ã¶zet Ã§ekme hatasÄ±: {e}")
        
        return None
    
    def _extract_general_abstract(self, soup):
        """Genel Ã¶zet Ã§ekme"""
        try:
            # Meta description
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                content = meta_desc['content'].strip()
                if len(content) > 50:
                    return content[:500] + "..." if len(content) > 500 else content
            
            # og:description
            og_desc = soup.find('meta', attrs={'property': 'og:description'})
            if og_desc and og_desc.get('content'):
                content = og_desc['content'].strip()
                if len(content) > 50:
                    return content[:500] + "..." if len(content) > 500 else content
                    
        except Exception as e:
            print(f"âŒ Genel Ã¶zet Ã§ekme hatasÄ±: {e}")
        
        return None


# Fallback iÃ§in sample articles
def get_sample_articles(query, limit=20):
    """Sample articles dÃ¶ndÃ¼r"""
    current_year = datetime.now().year
    
    articles = [
        {
            'id': 'sample_1',
            'title': f'{query.title()} Konusunda Akademik AraÅŸtÄ±rma',
            'authors': 'Prof. Dr. Akademik AraÅŸtÄ±rmacÄ±',
            'journal': 'International Journal of Research',
            'year': str(current_year),
            'abstract': f'Bu Ã§alÄ±ÅŸma {query} konusunda yapÄ±lan gÃ¼ncel akademik araÅŸtÄ±rmalarÄ± incelemektedir.',
            'detail_link': 'https://example.com/research',
            'pdf_link': 'https://www.mevzuat.gov.tr/File/GeneratePdf?mevzuatNo=4721&mevzuatTur=1&mevzuatTertip=5',
            'real_pdf': 'https://www.mevzuat.gov.tr/File/GeneratePdf?mevzuatNo=4721&mevzuatTur=1&mevzuatTertip=5',
            'doi': '',
            'source': 'Academic Database',
            'source_icon': 'ğŸ“š'
        },
        {
            'id': 'sample_2',
            'title': f'{query.title()} AlanÄ±nda LiteratÃ¼r TaramasÄ±',
            'authors': 'DoÃ§. Dr. LiteratÃ¼r UzmanÄ±',
            'journal': 'Journal of Academic Studies',
            'year': str(current_year),
            'abstract': f'{query} alanÄ±nda yapÄ±lan Ã§alÄ±ÅŸmalarÄ±n sistematik literatÃ¼r taramasÄ±.',
            'detail_link': 'https://example.com/literature',
            'pdf_link': 'https://www.mevzuat.gov.tr/File/GeneratePdf?mevzuatNo=6102&mevzuatTur=1&mevzuatTertip=5',
            'real_pdf': 'https://www.mevzuat.gov.tr/File/GeneratePdf?mevzuatNo=6102&mevzuatTur=1&mevzuatTertip=5',
            'doi': '',
            'source': 'Academic Database',
            'source_icon': 'ğŸ“š'
        },
        {
            'id': 'sample_3',
            'title': f'Modern {query.title()} YaklaÅŸÄ±mlarÄ±',
            'authors': 'Prof. Dr. Modern YaklaÅŸÄ±m UzmanÄ±',
            'journal': 'Contemporary Research Review',
            'year': str(current_year - 1),
            'abstract': f'{query} konusunda modern yaklaÅŸÄ±mlarÄ±n ve gÃ¼ncel geliÅŸmelerin analizi.',
            'detail_link': 'https://example.com/modern',
            'pdf_link': 'https://www.mevzuat.gov.tr/File/GeneratePdf?mevzuatNo=2004&mevzuatTur=1&mevzuatTertip=5',
            'real_pdf': 'https://www.mevzuat.gov.tr/File/GeneratePdf?mevzuatNo=2004&mevzuatTur=1&mevzuatTertip=5',
            'doi': '',
            'source': 'Academic Database',
            'source_icon': 'ğŸ“š'
        }
    ]
    
    return articles[:limit]