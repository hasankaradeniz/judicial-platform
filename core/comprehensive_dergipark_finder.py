import requests
import re
from urllib.parse import urljoin, urlparse
import logging
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

class ComprehensiveDergiParkFinder:
    """
    DergiPark iÃ§in kapsamlÄ± PDF bulma sistemi - tÃ¼m dergi formatlarÄ±nÄ± destekler
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def find_dergipark_pdfs(self, url, title, article_data, doi):
        """
        DergiPark'tan kapsamlÄ± PDF arama
        """
        pdf_links = []
        
        try:
            # DergiPark URL'ini oluÅŸtur/bul
            dergipark_url = self._construct_dergipark_url(url, doi, title)
            
            if dergipark_url:
                print(f"ðŸ“š DergiPark URL: {dergipark_url}")
                
                # DergiPark sayfasÄ±nÄ± getir
                response = self.session.get(dergipark_url, timeout=15)
                print(f"ðŸ“š DergiPark yanÄ±t: {response.status_code}")
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Ã‡oklu PDF arama stratejileri
                    pdf_links = self._comprehensive_pdf_search(soup, dergipark_url)
                    
                    print(f"ðŸ“š Toplam {len(pdf_links)} PDF bulundu")
                    
                else:
                    print(f"ðŸ“š DergiPark sayfasÄ± eriÅŸilemez: {response.status_code}")
            
            # URL bulunamazsa baÅŸlÄ±k ile arama
            if not pdf_links and title and len(title) > 10:
                print(f"ðŸ“š BaÅŸlÄ±k ile DergiPark aramasÄ± yapÄ±lÄ±yor...")
                search_results = self._search_by_title(title)
                pdf_links.extend(search_results)
        
        except Exception as e:
            print(f"ðŸ“š DergiPark arama hatasÄ±: {e}")
            logger.debug(f"DergiPark search error: {e}")
        
        return pdf_links[:3]  # Maksimum 3 PDF
    
    def _construct_dergipark_url(self, url, doi, title):
        """
        DergiPark URL'ini oluÅŸtur
        """
        # Mevcut URL varsa kullan
        if url and 'dergipark.org.tr' in url:
            return url
        
        # DOI'den URL oluÅŸtur
        if doi:
            # Bilinen DergiPark DOI patternleri
            doi_patterns = [
                # Sosyal GÃ¼venlik Dergisi
                (r'10\.32331/sgd\.(\d+)', r'https://dergipark.org.tr/tr/pub/sgd/article/\1'),
                # Genel 10.32331 pattern (TÃœBÄ°TAK dergileri)
                (r'10\.32331/([^.]+)\.(\d+)', r'https://dergipark.org.tr/tr/pub/\1/article/\2'),
                # 10.26466 pattern
                (r'10\.26466/([^.]+)\.(\d+)', r'https://dergipark.org.tr/tr/pub/\1/article/\2'),
                # 10.16953 pattern
                (r'10\.16953/([^.]+)\.(\d+)', r'https://dergipark.org.tr/tr/pub/\1/article/\2'),
                # 10.24014 pattern
                (r'10\.24014/([^.]+)\.(\d+)', r'https://dergipark.org.tr/tr/pub/\1/article/\2'),
                # Genel DergiPark pattern
                (r'10\.\d+/([^.]+)\.(\d+)', r'https://dergipark.org.tr/tr/pub/\1/article/\2'),
            ]
            
            for pattern, replacement in doi_patterns:
                match = re.search(pattern, doi)
                if match:
                    url = re.sub(pattern, replacement, doi)
                    print(f"ðŸ“š DOI'den URL oluÅŸturuldu: {doi} -> {url}")
                    return url
        
        return None
    
    def _comprehensive_pdf_search(self, soup, base_url):
        """
        KapsamlÄ± PDF arama stratejileri
        """
        pdf_links = []
        
        # Strateji 1: "Makale DosyalarÄ±" bÃ¶lÃ¼mÃ¼ arama
        pdf_links.extend(self._search_article_files_section(soup, base_url))
        
        # Strateji 2: "Tam Metin" direkt arama
        if not pdf_links:
            pdf_links.extend(self._search_full_text_links(soup, base_url))
        
        # Strateji 3: Download pattern arama
        if not pdf_links:
            pdf_links.extend(self._search_download_patterns(soup, base_url))
        
        # Strateji 4: Right sidebar arama
        if not pdf_links:
            pdf_links.extend(self._search_right_sidebar(soup, base_url))
        
        # Strateji 5: Genel PDF link arama
        if not pdf_links:
            pdf_links.extend(self._search_general_pdf_links(soup, base_url))
        
        return pdf_links
    
    def _search_article_files_section(self, soup, base_url):
        """
        "Makale DosyalarÄ±" bÃ¶lÃ¼mÃ¼nÃ¼ arama
        """
        pdf_links = []
        
        # Makale dosyalarÄ± baÅŸlÄ±k arama
        file_section_texts = [
            'Makale DosyalarÄ±', 'Article Files', 'Dosyalar', 'Files',
            'Tam Metin', 'Full Text', 'PDF', 'Ä°ndir', 'Download'
        ]
        
        for section_text in file_section_texts:
            # Section baÅŸlÄ±ÄŸÄ±nÄ± bul
            sections = soup.find_all(string=re.compile(section_text, re.IGNORECASE))
            
            for section in sections:
                parent = section.find_parent()
                if parent:
                    # Bu section'Ä±n altÄ±ndaki linkler
                    download_links = self._find_download_links_in_parent(parent)
                    
                    for link_info in download_links:
                        pdf_links.append({
                            'url': link_info['url'],
                            'type': f'section_{section_text.lower().replace(" ", "_")}',
                            'source': 'DergiPark',
                            'quality': 'high',
                            'found_method': f'Section: {section_text}'
                        })
                        print(f"ðŸ“š Section PDF bulundu ({section_text}): {link_info['url']}")
                    
                    if download_links:
                        break
            
            if pdf_links:
                break
        
        return pdf_links[:2]
    
    def _search_full_text_links(self, soup, base_url):
        """
        "Tam Metin" direkt link arama
        """
        pdf_links = []
        
        # Tam metin link arama
        full_text_patterns = [
            r'Tam\s+Metin', r'Full\s+Text', r'PDF\s+Ä°ndir', r'Download\s+PDF',
            r'Makale\s+PDF', r'Article\s+PDF', r'Ä°ndir', r'Download'
        ]
        
        for pattern in full_text_patterns:
            links = soup.find_all('a', string=re.compile(pattern, re.IGNORECASE))
            
            for link in links:
                href = link.get('href')
                if href and self._is_pdf_link(href):
                    pdf_url = self._make_absolute_url(href, base_url)
                    
                    pdf_links.append({
                        'url': pdf_url,
                        'type': 'full_text_direct',
                        'source': 'DergiPark',
                        'quality': 'high',
                        'found_method': f'Direct text: {pattern}'
                    })
                    print(f"ðŸ“š Tam Metin PDF bulundu: {pdf_url}")
                    break
            
            if pdf_links:
                break
        
        return pdf_links[:1]
    
    def _search_download_patterns(self, soup, base_url):
        """
        Download pattern arama
        """
        pdf_links = []
        
        # Download URL patternleri
        download_patterns = [
            r'/download/article-file/\d+',
            r'/download/[^/]+/\d+',
            r'/tr/download/[^"\']*',
            r'/en/download/[^"\']*',
        ]
        
        for pattern in download_patterns:
            links = soup.find_all('a', href=re.compile(pattern))
            
            for link in links[:3]:  # Ä°lk 3 link
                href = link.get('href')
                if href:
                    pdf_url = self._make_absolute_url(href, base_url)
                    
                    pdf_links.append({
                        'url': pdf_url,
                        'type': 'download_pattern',
                        'source': 'DergiPark',
                        'quality': 'high',
                        'found_method': f'Pattern: {pattern}'
                    })
                    print(f"ðŸ“š Pattern PDF bulundu: {pdf_url}")
                    
                    if len(pdf_links) >= 2:
                        break
            
            if pdf_links:
                break
        
        return pdf_links[:2]
    
    def _search_right_sidebar(self, soup, base_url):
        """
        Right sidebar arama
        """
        pdf_links = []
        
        # Right sidebar sÄ±nÄ±flarÄ±
        sidebar_selectors = [
            '.journal_panel_menu', '.right-panel', '.sidebar-right',
            '.article-sidebar', '.journal-sidebar', '.kt-portlet'
        ]
        
        for selector in sidebar_selectors:
            sidebars = soup.select(selector)
            
            for sidebar in sidebars:
                download_links = self._find_download_links_in_parent(sidebar)
                
                for link_info in download_links:
                    pdf_links.append({
                        'url': link_info['url'],
                        'type': 'sidebar',
                        'source': 'DergiPark',
                        'quality': 'high',
                        'found_method': f'Sidebar: {selector}'
                    })
                    print(f"ðŸ“š Sidebar PDF bulundu: {link_info['url']}")
                
                if download_links:
                    break
            
            if pdf_links:
                break
        
        return pdf_links[:2]
    
    def _search_general_pdf_links(self, soup, base_url):
        """
        Genel PDF link arama
        """
        pdf_links = []
        
        # Genel PDF iÃ§eren linkler
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href')
            link_text = link.get_text(strip=True).lower()
            
            # PDF belirten keywords
            pdf_keywords = ['pdf', 'tam metin', 'full text', 'download', 'indir', 'makale']
            
            if (self._is_pdf_link(href) or 
                any(keyword in link_text for keyword in pdf_keywords)):
                
                if self._is_pdf_link(href):
                    pdf_url = self._make_absolute_url(href, base_url)
                    
                    pdf_links.append({
                        'url': pdf_url,
                        'type': 'general_search',
                        'source': 'DergiPark',
                        'quality': 'medium',
                        'found_method': f'General: {link_text[:20]}...'
                    })
                    print(f"ðŸ“š Genel PDF bulundu: {pdf_url}")
                    
                    if len(pdf_links) >= 3:
                        break
        
        return pdf_links[:3]
    
    def _find_download_links_in_parent(self, parent):
        """
        Parent element iÃ§inde download linklerini bul
        """
        download_links = []
        
        # Download linki patternleri
        download_selectors = [
            'a[href*="/download/"]',
            'a[href*="article-file"]',
            'a[href*=".pdf"]',
        ]
        
        for selector in download_selectors:
            links = parent.select(selector)
            
            for link in links[:2]:  # Ä°lk 2 link
                href = link.get('href')
                if href and self._is_pdf_link(href):
                    download_links.append({
                        'url': href,
                        'text': link.get_text(strip=True)
                    })
        
        return download_links
    
    def _is_pdf_link(self, href):
        """
        Link'in PDF olup olmadÄ±ÄŸÄ±nÄ± kontrol et
        """
        if not href:
            return False
        
        href_lower = href.lower()
        
        # PDF belirten patternler
        pdf_indicators = [
            '/download/article-file/',
            '/download/',
            '.pdf',
            'pdf',
            'article-file'
        ]
        
        return any(indicator in href_lower for indicator in pdf_indicators)
    
    def _make_absolute_url(self, href, base_url):
        """
        Relative URL'yi absolute yap
        """
        if href.startswith('/'):
            return 'https://dergipark.org.tr' + href
        elif href.startswith('http'):
            return href
        else:
            return urljoin(base_url, href)
    
    def _search_by_title(self, title):
        """
        BaÅŸlÄ±k ile DergiPark arama
        """
        pdf_links = []
        
        try:
            search_url = "https://dergipark.org.tr/tr/search"
            params = {
                'q': title[:100],
                'section': 'articles'
            }
            
            response = self.session.get(search_url, params=params, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Ä°lk makale linklerini bul
                article_links = soup.find_all('a', href=re.compile(r'/tr/pub/[^/]+/article/\d+'))
                
                for article_link in article_links[:2]:  # Ä°lk 2 sonuÃ§
                    article_url = 'https://dergipark.org.tr' + article_link['href']
                    print(f"ðŸ“š Arama sonucu makale: {article_url}")
                    
                    # Bu makale sayfasÄ±ndan PDF Ã§ek
                    search_pdfs = self.find_dergipark_pdfs(article_url, title, {}, None)
                    pdf_links.extend(search_pdfs)
                    
                    if pdf_links:
                        break
        
        except Exception as e:
            print(f"ðŸ“š BaÅŸlÄ±k arama hatasÄ±: {e}")
        
        return pdf_links

def find_comprehensive_dergipark_pdfs(url, title, article_data, doi):
    """
    Ana fonksiyon - kapsamlÄ± DergiPark PDF arama
    """
    finder = ComprehensiveDergiParkFinder()
    return finder.find_dergipark_pdfs(url, title, article_data, doi)