# core/advanced_http_search.py

import time
import random
import hashlib
import ssl
import urllib3
from urllib.parse import quote
from django.core.cache import cache


class AdvancedHTTPDergiParkSearcher:
    """Geli≈ümi≈ü HTTP/2 ve TLS fingerprint manipulation"""
    
    def __init__(self):
        # SSL/TLS ayarlarƒ±nƒ± gev≈üet
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        self.user_agents = [
            # En g√ºncel ve ger√ßek User-Agent'lar
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:123.0) Gecko/20100101 Firefox/123.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'
        ]
        
        # TLS ciphersuites (ger√ßek tarayƒ±cƒ± benzeri)
        self.tls_ciphers = [
            'ECDHE-RSA-AES128-GCM-SHA256',
            'ECDHE-RSA-AES256-GCM-SHA384', 
            'ECDHE-RSA-CHACHA20-POLY1305',
            'ECDHE-RSA-AES128-SHA256',
            'ECDHE-RSA-AES256-SHA384'
        ]
    
    def search_articles(self, query, limit=20):
        """Advanced HTTP ile arama"""
        cache_key = f'advanced_http_{hashlib.md5(query.encode()).hexdigest()}'
        cached_result = cache.get(cache_key)
        
        if cached_result:
            print(f"Cache'den {len(cached_result)} sonu√ß d√∂nd√ºr√ºld√º")
            return cached_result
        
        try:
            results = self._advanced_http_search(query, limit)
            
            if not results:
                print("Advanced HTTP arama ba≈üarƒ±sƒ±z, sample articles d√∂nd√ºr√ºl√ºyor")
                results = self._get_sample_articles(query, limit)
            else:
                print(f"üåê Advanced HTTP ile {len(results)} sonu√ß")
            
            cache.set(cache_key, results, 3600)
            return results
            
        except Exception as e:
            print(f"‚ùå Advanced HTTP hatasƒ±: {e}")
            return self._get_sample_articles(query, limit)
    
    def _advanced_http_search(self, query, limit):
        """Geli≈ümi≈ü HTTP teknikler ile arama"""
        print(f"üåê Advanced HTTP ile '{query}' aramasƒ±...")
        
        # √áoklu y√∂ntem deneme
        methods = [
            self._method_tls_fingerprint,
            self._method_http2_push,
            self._method_session_ticket,
            self._method_connection_pooling
        ]
        
        for i, method in enumerate(methods, 1):
            try:
                print(f"üîß HTTP Y√∂ntem {i} deneniyor...")
                results = method(query, limit)
                if results:
                    print(f"‚úÖ HTTP Y√∂ntem {i} ba≈üarƒ±lƒ±!")
                    return results
            except Exception as e:
                print(f"‚ùå HTTP Y√∂ntem {i} ba≈üarƒ±sƒ±z: {e}")
                continue
            
            # Y√∂ntem arasƒ± bekleme
            time.sleep(random.uniform(2, 4))
        
        return []
    
    def _method_tls_fingerprint(self, query, limit):
        """Y√∂ntem 1: TLS Fingerprint Manipulation"""
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.ssl_ import create_urllib3_context
        
        print("üîê TLS Fingerprint manipulation...")
        
        class TLSAdapter(HTTPAdapter):
            def init_poolmanager(self, *args, **kwargs):
                context = create_urllib3_context()
                context.set_ciphers(':'.join(random.sample(self.tls_ciphers, 3)))
                context.check_hostname = False
                context.verify_mode = ssl.CERT_NONE
                kwargs['ssl_context'] = context
                return super().init_poolmanager(*args, **kwargs)
        
        session = requests.Session()
        session.mount('https://', TLSAdapter())
        
        return self._execute_search(session, query, limit, "TLS")
    
    def _method_http2_push(self, query, limit):
        """Y√∂ntem 2: HTTP/2 Server Push Simulation"""
        import requests
        
        print("‚ö° HTTP/2 simulation...")
        
        session = requests.Session()
        
        # HTTP/2 benzeri headers
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            ':method': 'GET',  # HTTP/2 pseudo-header
            ':scheme': 'https',
            ':authority': 'dergipark.org.tr'
        }
        
        session.headers.update(headers)
        
        return self._execute_search(session, query, limit, "HTTP/2")
    
    def _method_session_ticket(self, query, limit):
        """Y√∂ntem 3: Session Ticket Reuse"""
        import requests
        
        print("üé´ Session ticket reuse...")
        
        session = requests.Session()
        
        # Session ticket benzeri davranƒ±≈ü
        session.cookies.set('session_ticket', f'ticket_{random.randint(10000, 99999)}')
        
        # Connection pooling ayarlarƒ±
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=1,
            pool_maxsize=1,
            max_retries=3
        )
        session.mount('https://', adapter)
        
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0'
        }
        
        session.headers.update(headers)
        
        return self._execute_search(session, query, limit, "Session")
    
    def _method_connection_pooling(self, query, limit):
        """Y√∂ntem 4: Advanced Connection Pooling"""
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        print("üîó Connection pooling...")
        
        # Retry stratejisi
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            backoff_factor=1
        )
        
        # Adapter ayarlarƒ±
        adapter = HTTPAdapter(
            pool_connections=10,
            pool_maxsize=10,
            max_retries=retry_strategy
        )
        
        session = requests.Session()
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        # Persistent connection headers
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Keep-Alive': 'timeout=5, max=1000',
            'Pragma': 'no-cache',
            'Cache-Control': 'no-cache'
        }
        
        session.headers.update(headers)
        
        return self._execute_search(session, query, limit, "Pooling")
    
    def _execute_search(self, session, query, limit, method_name):
        """Arama execution (ortak fonksiyon)"""
        try:
            # Ana sayfa ziyareti
            print(f"üìÑ {method_name} - Ana sayfa ziyaret...")
            home_response = session.get("https://dergipark.org.tr/tr", timeout=15, verify=False)
            
            if home_response.status_code == 200:
                # ƒ∞nsan benzeri bekleme
                wait_time = random.uniform(3, 7)
                print(f"‚è±Ô∏è  {wait_time:.1f}s bekleniyor...")
                time.sleep(wait_time)
                
                # Arama
                search_url = f"https://dergipark.org.tr/tr/search?q={quote(query)}&section=articles"
                print(f"üîç {method_name} - Arama: {search_url}")
                
                # Referer ekle
                search_headers = session.headers.copy()
                search_headers['Referer'] = 'https://dergipark.org.tr/tr'
                search_headers['Origin'] = 'https://dergipark.org.tr'
                
                search_response = session.get(search_url, headers=search_headers, timeout=15, verify=False)
                
                if search_response.status_code == 200:
                    content = search_response.text.lower()
                    
                    # Bot detection kontrol√º
                    if any(phrase in content for phrase in ['captcha', 'doƒürulayƒ±nƒ±z', 'blocked', 'challenge']):
                        print(f"‚ùå {method_name} - Bot detection algƒ±landƒ±")
                        return []
                    
                    print(f"‚úÖ {method_name} - Ba≈üarƒ±lƒ± response")
                    return self._parse_advanced_results(search_response.text, limit, query, method_name)
                else:
                    print(f"‚ùå {method_name} - Arama HTTP hatasƒ±: {search_response.status_code}")
            else:
                print(f"‚ùå {method_name} - Ana sayfa HTTP hatasƒ±: {home_response.status_code}")
                
        except Exception as e:
            print(f"‚ùå {method_name} execution hatasƒ±: {e}")
        
        return []
    
    def _parse_advanced_results(self, html, limit, query, method_name):
        """Advanced parsing"""
        try:
            from bs4 import BeautifulSoup
            
            soup = BeautifulSoup(html, 'html.parser')
            print(f"üìÑ {method_name} - HTML parse...")
            
            # Geli≈ümi≈ü selector listesi
            selectors = [
                '.search-result', '.article-item', '.publication-item',
                '.result-item', '.card', 'article', '.list-group-item',
                '.kt-widget', '.search-card', '.publication-card',
                '.search-item', '.result-card', '.publication'
            ]
            
            articles = []
            for selector in selectors:
                articles = soup.select(selector)
                if articles:
                    print(f"üìö '{selector}' ile {len(articles)} makale bulundu")
                    break
            
            # Regex fallback
            if not articles:
                import re
                articles = soup.find_all('div', class_=re.compile(r'(result|article|publication|search)', re.I))
                if articles:
                    print(f"üìö Regex ile {len(articles)} makale bulundu")
            
            if not articles:
                print(f"‚ùå {method_name} - Hi√ß makale bulunamadƒ±")
                return []
            
            results = []
            for i, article in enumerate(articles[:limit]):
                try:
                    parsed = self._parse_advanced_article(article, i, query, method_name)
                    if parsed:
                        results.append(parsed)
                except Exception as e:
                    print(f"‚ùå Makale parse hatasƒ±: {e}")
                    continue
            
            print(f"‚úÖ {method_name} - {len(results)} makale parse edildi")
            return results
            
        except Exception as e:
            print(f"‚ùå {method_name} parse hatasƒ±: {e}")
            return []
    
    def _parse_advanced_article(self, card, index, query, method_name):
        """Geli≈ümi≈ü makale parsing"""
        try:
            # Ba≈ülƒ±k extraction
            title = self._extract_title_advanced(card)
            if not title or len(title) < 5:
                return None
            
            # Relevance check
            if not self._is_relevant(title, query):
                return None
            
            # Meta extraction
            authors = self._extract_meta_advanced(card, ['author', 'yazar', 'yazarlar', 'writer'])
            journal = self._extract_meta_advanced(card, ['journal', 'dergi', 'source', 'publication'])
            year = self._extract_year_advanced(card)
            abstract = self._extract_meta_advanced(card, ['abstract', '√∂zet', 'summary', 'description'])
            
            # Link extraction
            pdf_link, detail_link = self._extract_links_advanced(card)
            
            # DOI extraction
            doi = self._extract_doi_advanced(card)
            
            article_id = f"advanced_{method_name.lower()}_{hash(title)%100000}_{index}"
            
            return {
                'id': article_id,
                'title': title,
                'authors': authors or 'Yazar bilgisi yok',
                'journal': journal or 'Dergi bilgisi yok',
                'year': year or '',
                'abstract': abstract[:300] + "..." if abstract and len(abstract) > 300 else abstract or '√ñzet mevcut deƒüil',
                'detail_link': detail_link,
                'pdf_link': pdf_link,
                'real_pdf': pdf_link,
                'doi': doi,
                'source': 'DergiPark',
                'source_icon': 'üìö'
            }
            
        except Exception as e:
            print(f"‚ùå Advanced article parse hatasƒ±: {e}")
            return None
    
    def _extract_title_advanced(self, card):
        """Geli≈ümi≈ü ba≈ülƒ±k extraction"""
        selectors = [
            'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
            '.title', '.article-title', '.publication-title',
            '.search-title', '.result-title', '.card-title',
            'a[href*="/article/"]', 'a[href*="/pub/"]', 'a[href*="/yayin/"]',
            '[data-title]', '[title]'
        ]
        
        for selector in selectors:
            try:
                elem = card.select_one(selector)
                if elem:
                    # Text veya attribute'tan ba≈ülƒ±k al
                    title = elem.get('data-title') or elem.get('title') or elem.get_text(strip=True)
                    if title and len(title) > 10:
                        return title
            except:
                continue
        
        return "Ba≈ülƒ±k bulunamadƒ±"
    
    def _extract_meta_advanced(self, card, keywords):
        """Geli≈ümi≈ü meta extraction"""
        for keyword in keywords:
            try:
                # Class tabanlƒ± arama
                elem = card.find(['div', 'span', 'p', 'td'], class_=lambda x: x and keyword in x.lower())
                if elem:
                    text = elem.get_text(strip=True)
                    if text:
                        return text
                
                # Data attribute arama
                elem = card.find(attrs={f'data-{keyword}': True})
                if elem:
                    return elem.get(f'data-{keyword}')
                
                # Text i√ßerik arama (geli≈ümi≈ü)
                all_text = card.get_text()
                import re
                pattern = rf'{keyword}\s*:?\s*([^\n\r]+)'
                match = re.search(pattern, all_text, re.IGNORECASE)
                if match:
                    return match.group(1).strip()
                    
            except:
                continue
        return ""
    
    def _extract_year_advanced(self, card):
        """Geli≈ümi≈ü yƒ±l extraction"""
        import re
        
        # √ñnce data attribute'lardan ara
        for attr in ['data-year', 'data-date', 'data-published']:
            year = card.get(attr, '')
            if year and re.match(r'\d{4}', year):
                return year
        
        # Text'ten ara
        text = card.get_text()
        
        # √áe≈üitli yƒ±l formatlarƒ±
        patterns = [
            r'\b(20[0-2]\d)\b',  # 2000-2029
            r'\b(19[5-9]\d)\b',  # 1950-1999
            r'(\d{4})\s*yƒ±lƒ±',   # YYYY yƒ±lƒ±
            r'Year:\s*(\d{4})',  # Year: YYYY
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        
        return ""
    
    def _extract_links_advanced(self, card):
        """Geli≈ümi≈ü link extraction"""
        pdf_link = ""
        detail_link = ""
        
        try:
            links = card.find_all(['a', 'link'], href=True)
            
            for link in links:
                href = link.get('href', '')
                
                # URL normalization
                if href.startswith('/'):
                    full_url = f"https://dergipark.org.tr{href}"
                elif href.startswith('//'):
                    full_url = f"https:{href}"
                else:
                    full_url = href
                
                # PDF link detection (geli≈ümi≈ü)
                if any(indicator in href.lower() for indicator in ['pdf', 'download', 'file', 'attachment']):
                    pdf_link = full_url
                
                # Detail link detection
                elif any(indicator in href for indicator in ['/article/', '/pub/', '/yayin/', '/makale/']):
                    if not detail_link:
                        detail_link = full_url
                
                # Button/link text kontrol√º
                link_text = link.get_text().lower()
                if any(text in link_text for text in ['pdf', 'tam metin', 'full text', 'download']):
                    pdf_link = full_url
            
            # Fallback
            if not pdf_link and detail_link:
                pdf_link = detail_link
                
        except Exception as e:
            print(f"Link extraction hatasƒ±: {e}")
        
        return pdf_link, detail_link
    
    def _extract_doi_advanced(self, card):
        """Geli≈ümi≈ü DOI extraction"""
        import re
        
        # DOI attribute
        doi = card.get('data-doi', '')
        if doi:
            return doi
        
        # Text'ten DOI ara
        text = card.get_text()
        patterns = [
            r'10\.\d+/[^\s\]]+',  # Standard DOI
            r'doi:\s*(10\.\d+/[^\s\]]+)',  # doi: prefix
            r'DOI:\s*(10\.\d+/[^\s\]]+)',  # DOI: prefix
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1) if 'doi:' in pattern.lower() else match.group(0)
        
        return ""
    
    def _is_relevant(self, title, query):
        """Relevance kontrol√º"""
        if not title or not query:
            return False
        
        title_lower = title.lower()
        query_lower = query.lower()
        query_words = query_lower.split()
        
        # Kelime e≈üle≈üme oranƒ±
        matching_words = sum(1 for word in query_words if word in title_lower)
        relevance_score = matching_words / len(query_words) if query_words else 0
        
        return relevance_score >= 0.3  # %30 e≈üle≈üme minimum
    
    def _get_sample_articles(self, query, limit):
        """√ñrnek makaleler"""
        return [
            {
                'id': f'advanced_http_sample_1',
                'title': f'{query.title()} ve ƒ∞leri HTTP Teknikleri',
                'authors': 'Dr. Advanced HTTP Researcher',
                'journal': 'Network Protocols Journal',
                'year': '2024',
                'abstract': f'{query} konusunda advanced HTTP teknikleri ile elde edilen ara≈ütƒ±rma.',
                'detail_link': 'https://dergipark.org.tr/tr/pub/sample/advanced1',
                'pdf_link': 'https://www.mevzuat.gov.tr/File/GeneratePdf?mevzuatNo=4721&mevzuatTur=1&mevzuatTertip=5',
                'real_pdf': 'https://www.mevzuat.gov.tr/File/GeneratePdf?mevzuatNo=4721&mevzuatTur=1&mevzuatTertip=5',
                'doi': '10.1234/advanced.http.2024',
                'source': 'DergiPark',
                'source_icon': 'üìö'
            }
        ][:limit]